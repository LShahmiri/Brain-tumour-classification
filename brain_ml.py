# -*- coding: utf-8 -*-
"""Brain_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bKC0twCAOWlaqTItrIOsOQCutnm3W5Db
"""

# =========================================================
# Brain Tumor Classification using Xception (ImageNet)
# Binary Classification: Tumor vs No Tumor
# =========================================================

# --------- Imports ---------
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive

# --------- Mount Google Drive ---------
drive.mount('/content/drive')

# --------- Configuration ---------
BASE_DIR = "/content/drive/MyDrive/DATASETS/Brain Tumor Data Set"
IMG_SIZE = (224, 224)          # Required size for Xception
BATCH_SIZE = 32
EPOCHS = 100
SEED = 42

# --------- Load Dataset (Train / Validation Split) ---------
train_ds = tf.keras.utils.image_dataset_from_directory(
    BASE_DIR,
    validation_split=0.2,
    subset="training",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="binary"
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    BASE_DIR,
    validation_split=0.2,
    subset="validation",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="binary"
)

class_names = train_ds.class_names
print("Class names:", class_names)

# Improve input pipeline performance
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
val_ds = val_ds.cache().prefetch(AUTOTUNE)

# --------- Data Augmentation ---------
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.15),
    layers.RandomContrast(0.1),
], name="data_augmentation")

# Xception preprocessing (ImageNet normalization)
preprocess_input = tf.keras.applications.xception.preprocess_input

# --------- Base Model: Xception (Pretrained on ImageNet) ---------
base_model = tf.keras.applications.Xception(
    weights="imagenet",
    include_top=False,
    input_shape=(224, 224, 3)
)

# Freeze base model for transfer learning
base_model.trainable = False

# --------- Build Full Model ---------
inputs = keras.Input(shape=(224, 224, 3))

x = data_augmentation(inputs)
x = preprocess_input(x)

x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)

model = keras.Model(inputs, outputs)

# --------- Compile Model ---------
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss="binary_crossentropy",
    metrics=[
        "accuracy",
        tf.keras.metrics.AUC(name="auc")
    ]
)

model.summary()

# --------- Callbacks ---------
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=6,
        restore_best_weights=True
    ),
    keras.callbacks.ModelCheckpoint(
        filepath="/content/drive/MyDrive/DATASETS/model-saved/xception_best_model.h5",
        monitor="val_loss",
        save_best_only=True
    )
]

# --------- Training (Up to 100 Epochs) ---------
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=callbacks
)

# --------- Save Models (H5 format) ---------
base_model.save("/content/drive/MyDrive/DATASETS/model-saved/xception_base_model.h5")
model.save("/content/drive/MyDrive/DATASETS/model-saved/xception_full_model.h5")
print("Models saved as .h5 files")

# --------- Plot Training / Validation Loss and Accuracy ---------
plt.figure(figsize=(12, 4))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()

# Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Training vs Validation Accuracy")
plt.legend()

plt.show()

# --------- Confusion Matrix on Validation Set ---------
y_true = []
y_pred = []

for x_batch, y_batch in val_ds:
    probs = model.predict(x_batch, verbose=0)
    preds = (probs >= 0.5).astype(int)

    y_true.extend(y_batch.numpy().astype(int))
    y_pred.extend(preds.flatten())

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(5, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=class_names,
    yticklabels=class_names
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix (Validation Set)")
plt.show()

print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

# --------- Single Example Inference from Validation ---------
for images, labels in val_ds.take(1):
    img = images[0]
    true_label = int(labels[0].numpy())

    prob = model.predict(tf.expand_dims(img, 0), verbose=0)[0][0]
    pred_label = int(prob >= 0.5)

    plt.imshow(img.numpy().astype("uint8"))
    plt.title(
        f"True: {class_names[true_label]} | "
        f"Predicted: {class_names[pred_label]} | "
        f"Probability: {prob:.3f}"
    )
    plt.axis("off")
    plt.show()